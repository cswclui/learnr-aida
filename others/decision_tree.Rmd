# Decision Trees

```{r include = FALSE}
#source("_common.R")

knitr::opts_chunk$set(warning = FALSE)
```

One of our favorite machine learning algorithms is decision trees. They are not the most complex models, but they are quite intuitive. A single decision tree generally doesn't have great "out of the box" model performance, and even with considerable model tuning they are unlikely to perform as well as other approaches. They do, however, form the building blocks for more complicated models that *do* have high out-of-the-box model performance and can produce state-of-the-art level predictions.

Decision trees are non-parametric, meaning they do not make any assumptions about the underlying data generating process. By contrast, models like linear regression assume that the underlying data were generated by a standard normal distribution (and, if this is not the case, the model will result is systematic biases - although we can also use transformations and other strategies to help; see [Feature Engineering]). Note that assuming an underlying data generating distribution is not a *weakness* of linear regression - often it's a tenable assumption and can regularly lead to better model performance, if the assumption holds. But decision trees do not require that you make any such assumptions, which is particularly helpful when it's difficult to assume a specific underlying data generating distribution.

At their core, decision trees work by *splitting* the features into a series of yes/no decisions. These splits divide the feature space into a series of non-overlapping regions, where the cases are similar in each region. To understand how a given prediction is made, one simply "follows" the splits of the tree (a branch) to the terminal node (the final prediction). This splitting continues until a specified criterion is met.

### A simple decision tree

Initially, we think it's easiest to think about decision trees through a classification lens. One of the most common example datasets for decision trees is the *titanic* dataset, which includes information on passengers aboard the titanic. The data look like this

<!-- Note - the data below is from kaggle: https://www.kaggle.com/c/titanic/data?select=train.csv -->

```{r }
library(tidyverse)
library(titanic)
titanic<-titanic_train
titanic$Survived <- factor(titanic$Survived,
                           levels = c(0, 1),
                           labels = c("Died", "Survived"))

head(titanic)
```

Imagine we wanted to create a model that would predict if passengers aboard the titanic survived.

A decision tree model might look something like this.

```{r echo = FALSE}
library(rpart)
library(rpart.plot)

# Generate the decision tree
m <- rpart(Survived ~ Sex + Age + SibSp, data = titanic)

#plot the decision tree
rpart.plot(m, type = 4)


```

where `Sibsp` indicates the number of siblings/spouses onboard with the passenger.

-   In the above, there is a **root node** at the top that provides us some descriptive statistics - namely that when there have been no splits on any features (we have 100% of the sample), approximately 38% of passengers survived.

-   The first feature we split on is the sex of the passenger, which in this case is a binary indicator with two levels: `male` and `female`. Approximately 65% of all passengers were coded as male, while the remaining 35% were coded as female. Of those that were coded male, about 19% survived, while approximately 74% of those coded female survived.

-   These are different **branches** of the tree. Each node is then split again on an **internal node**.

-   For passengers coded female, we use the number of siblings/spouses, with an optimal cut being three. Those with three or fewer would be predicted to survive, while those with two or less would be not be predicted to survive. Note that these are the final predictions, or the **terminal nodes** for this branch.

-   Females with three or fewer siblings/spouses represent 33% of the total sample, of which 77% survived (as predicted by the terminal node), while females with three or more siblings/spouses represent 2% of the total sample, of which 29% survived (these passengers would not be predicted to survive).

-   For male passengers, the first internal node splits first on age, because this is the more important feature for these passengers. Those who were six and a half or older are immediately predicted to not survive. This group represents 62% of the total sample, with only a 17% survival rate.

-   However, for passengers coded male who were younger than 6.5, there was a small amount of additional information in the siblings/spouses feature. Passengers with three or more siblings/spouses would not be predicted to survive (representing 1% of the total sample, and an 11% survival rate) while those with fewer than three sibling/spouses would be predicted to survive (and all such passengers did actually survive, representing 2% of the total sample).

-   Note that in this example, the optimal split for siblings/spouses happened to be the same in both branches, but this is a bit of coincidence. It's possible there's something important about this number, but the split value does not have to be the same, and in fact the same feature can be used multiple times for multiple splits, each with a different value, while splitting on internal nodes.

-   For classification problems, like the above, the predicted class in the terminal node is determined by the most frequent class (the mode). For regression problems, the prediction is determined by the mean of the outcome for all cases in the given terminal node.

| Terms                                  | Description                                                                                                                                                        |
|:------------------|:----------------------------------------------------|
| **Root node**                          | The top feature in a decision tree. The column that has the first split                                                                                            |
| **Internal node**                      | A grouping within the tree between the root node and the terminal node, e.g., *all passengers coded female*.                                                       |
| **Terminal node** or **Terminal leaf** | The final node/leaf of a decision tree. The prediction grouping.                                                                                                   |
| **Branch**                             | The prediction "flow" of the tree. One often "follows" a branch to a terminal node.                                                                                |
| **Split**                              | A threshold value for numeric features, or a classification separation rule for categorical features, that optimally separates the sample according to the outcome |





Here is the decision tree if we use only one split.

```{r echo = FALSE}
library(rpart)
library(rpart.plot)

# Generate the decision tree
m <- rpart(Survived ~ Sex + Age + SibSp, data = titanic,
                 control = list(maxdepth = 1))

#plot the decision tree
#adding extra=1 parameter will display the number of observations that fall in the node
rpart.plot(m, type = 4)
rpart.plot(m, type = 4, extra=1)

```
From the decision tree

- Passengers have 0.38 probability of survival overall.
- Male has 19% survival while female has 74% survival


Here is the decision tree if we use two splits.


```{r echo = FALSE}
library(rpart)
library(rpart.plot)

# Generate the decision tree
m <- rpart(Survived ~ Sex + Age + SibSp, data = titanic,
                 control = list(maxdepth = 2))

#plot the decision tree
#adding extra=1 parameter will display the number of observations that fall in the node
rpart.plot(m, type = 4)
rpart.plot(m, type = 4, extra=1)

```



The complexity parameter (cp) in rpart is the minimum improvement in the model needed at each node. 

```{r echo = TRUE, fig.width=10,fig.height=6}
library(rpart)
library(rpart.plot)

# Generate the decision tree
m <- rpart(Survived ~ Sex + Age + SibSp, data = titanic,
                 control = list(cp=0))

#plot the decision tree
rpart.plot(m, type = 4) 
rpart.plot(m, type = 4, extra=1) 
```


## Visualizing decision trees

The decision tree itself is often helpful for understanding how predictions are made. Scatterplots can also be helpful in terms of viewing how the predictor space is being partitioned. For example, imagine we are fitting a model to the following data

```{r }
ggplot(mpg, aes(displ, cty)) +
  geom_point()
```

In this case, we only have a single predictor variable, `displ`, but we can split on that variable multiple times. Let's start with a single split, and we'll build from there. The decision tree for a single split model (also called a "stump") looks like this

```{r echo = FALSE}
displ_stump <- rpart(cty ~ displ, mpg,
                 control = list(maxdepth = 1))
rpart.plot(displ_stump, type = 4)
```

So we are just splitting based on whether `displ` is greater than or equal to 2.6. 
How does this look in the scatterplot? Well, if the `displ >= 2.6`, it's a horizontal line at 14, and if `displ <= 2.6`, it's a horizontal line at 21.

```{r echo = FALSE}
unique_displ <- data.frame(displ = unique(mpg$displ))

stump_predframe <- unique_displ %>% 
  mutate(cty = predict(displ_stump, newdata = .))

stump_predframe

ggplot(mpg, aes(displ, cty)) +
  geom_point() +
  geom_line(data = stump_predframe)
```

What if we add one additional split? Well then the decision tree looks like this

```{r echo = FALSE}
displ_twosplit <- rpart(cty ~ displ, mpg,
                 control = list(maxdepth = 2))
rpart.plot(displ_twosplit, type = 4)
```

and the scatterplot looks like this

```{r echo = FALSE}
twosplit_predframe <- unique_displ %>% 
  mutate(cty = predict(displ_twosplit, newdata = .))

ggplot(mpg, aes(displ, cty)) +
  geom_point() +
  geom_line(data = twosplit_predframe)
```

As the number of splits increases, the fit to the data increases. However, the model can also quickly overfit and not generalize to new data well, which is why a single decision tree is often not the most performant model.

We can visualize classification problems with decision trees using scatterplots as well, as long as there are two continuous predictor variables. Sticking with our `mpg` dataset, let's say we wanted to predict the drivetrain: front-wheel drive, rear-wheel drive, or four-wheel drive. The scatterplot would then look like this (assuming we're now using `displ` and `cty` as predictor variables).

```{r }
ggplot(mpg, aes(displ, cty)) +
  geom_point(aes(color = drv))
```

How does the predictor space get divided up? Let's try first with a stump model. The tree looks like this

```{r }
drv_stump <- rpart(drv ~  displ + cty, mpg,
                 control = list(maxdepth = 1))
rpart.plot(drv_stump, type = 4)
```

This model isn't doing too well. It looks like we're not capturing any of the rear-wheel drive cases. That of course makes sense, because we only have one split, so we can't predict three different classes. Here's what it looks like with the scatterplot

**NOTE:** There's a bug (I think) in the package that currently means I have to flip the axes here. We should come back and change it when it's fixed. See here: <https://github.com/grantmcdermott/parttree/issues/5>


```{r}
mpg$drv <- as.factor(mpg$drv)
str(mpg$drv)
```



```{r }
# install.packages("remotes")
#remotes::install_github("grantmcdermott/parttree")
library(parttree)
ggplot(mpg, aes(cty, displ)) +
  geom_parttree(aes(fill = drv), 
                data = drv_stump, 
                alpha = 0.4,
                flipaxes = TRUE) +
  geom_point(aes(color = drv))
```

And what if we go with a more complicated model? Say, 3 splits? Then the tree looks like this.

```{r }
drv_threesplits <- rpart(drv ~  displ + cty, mpg,
                 control = list(maxdepth = 3))
rpart.plot(drv_threesplits, type = 4)
```

And our predictor space can now be divided up better.

```{r }
ggplot(mpg, aes(displ, cty)) +
  geom_parttree(aes(fill = drv), 
                data = drv_threesplits, 
                alpha = 0.4,
                flipaxes = TRUE) +
  geom_point(aes(color = drv))
```

This looks considerably better and, although we still have some misclassification, that's ultimately probably a good thing because we don't want to start to overfit to our training data. Decision trees, like all models, should balance the bias-variance tradeoff. They are flexible and make few assumptions about the data, but that can also quickly lead to overfitting and poor generalizations to new data.

# References

- https://www.sds.pub/decision-trees.html#a-simple-decision-tree
- https://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/





## Determining optimal splits

Regression trees work by optimally splitting each node until some criterion is met. But how do we determine what's optimal? We use an objective function. For regression problems, this usually the sum of the squared errors, defined by

$$
\Sigma_{R1}(y_i - c_1)^2 + \Sigma_{R2}(y_i - c_2)^2
$$

where $c$ is the prediction (mean of cases in the region), which starts as a constant and is updated, and $R$ is the region. The algorithm searches through every possible split for every possible feature to identify the split that minimizes the sum of the squared errors.

For classification problems, the Gini impurity index is most common. Note that this is *not* the same index that is regularly used in spatial demography and similar fields to estimate inequality. Confusingly, both terms are referred to as the Gini index (with the latter also being referred to as the Gini coefficient or Gini ratio), but they are entirely separate.

For a two-class situation, the Gini impurity index, used in decision trees, is defined by

$$
D_i = 1 - P(c_1)^2 - P(c_2)^2
$$

where where $P(c_1)$ and $P(c_2)$ are the probabilities of being in Class 1 or 2, respectively, for node $i$. This formula can be generalized to a multiclass situation by

$$
D_i = 1 - \Sigma(p_i)^2
$$

In either case, when $D = 0$, the node is "pure" and the classification is perfect. When $D = 0.5$, the node is random (flip a coin). As an example, consider a terminal node with 75% of cases in one class. The Gini impurity index would be estimated as

$$
\begin{aligned}
D &= 1 - (0.75^2 + 0.25^2) \\\
D &= 1 - (0.5625 + 0.0625) \\\
D &= 1 - 0.625 \\\
D &= 0.375
\end{aligned}
$$

As the proportion in one class goes up, the value of $D$ goes down. Similar to regression problems, the searches through all possible features to find the optimal split that minimizes $D$.

Regardless of whether the decision tree is built to solve a regression or classification problem, the algorithm is built recursively, with new optimal splits determined from previous splits. This "top down" approach is one example of a **greedy algorithm**, in which a series of *localized* optimal decisions are made. In other words, the optimal split is determined for each node. There is no going backwards to check if a different *combination* of features and splits would lead to better performance.



# test only

Examine the outcome variable.

```{r}
levels(titanic$Survived)
#as.numeric(levels(titanic$Survived)[f])
titanic[1:10,]$Survived
as.numeric(titanic[1:10,]$Survived)

```


```{r}
table(titanic$Survived)/nrow(titanic)
```

Decision Tree with 1 split.

```{r}
library(parsnip)
library(titanic) 

titanic_train$Survived = as.factor(titanic_train$Survived)

# Generate the decision tree
model <- rpart(Survived ~ Pclass+Age, data = titanic_train,
                 control = list(maxdepth = 1)) #change maxdepth = 2 and 3 to observe the decision boundary
model
rpart.plot(model, type = 4)


## Plot the data and model partitions
titanic_train %>%
  ggplot(aes(x=Pclass, y=Age)) +
  geom_jitter(aes(col=Survived), alpha=0.7) +
  geom_parttree(data = model, aes(fill=Survived), alpha = 0.1) +
  theme_minimal()

```

Decision tree with two splits.

```{r}
library(parsnip)
library(titanic) 

titanic_train$Survived = as.factor(titanic_train$Survived)

# Generate the decision tree
model <- rpart(Survived ~ Pclass+Age, data = titanic_train,
                 control = list(maxdepth = 2)) #change maxdepth = 2 and 3 to observe the decision boundary

rpart.plot(model, type = 4)

## Plot the data and model partitions
titanic_train %>%
  ggplot(aes(x=Pclass, y=Age)) +
  geom_jitter(aes(col=Survived), alpha=0.7) +
  geom_parttree(data = model, aes(fill=Survived), alpha = 0.1) +
  theme_minimal()

```



```{r}
library(tidymodels)
library(parsnip)
library(titanic) 

titanic_train$Survived = as.factor(titanic_train$Survived)

## Build our tree using parsnip (but with rpart as the model engine)
ti_tree =
  decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(Survived ~ Pclass + Age, data = titanic_train)

ti_tree$fit

rpart.plot(ti_tree$fit, type = 4)

## Plot the data and model partitions
titanic_train %>%
  ggplot(aes(x=Pclass, y=Age)) +
  geom_jitter(aes(col=Survived), alpha=0.7) +
  geom_parttree(data = ti_tree, aes(fill=Survived), alpha = 0.1) +
  theme_minimal()
```


```{r}
library(parsnip)
library(titanic) 

titanic_train$Survived = as.factor(titanic_train$Survived)

## Build our tree using parsnip (but with rpart as the model engine)
model =
  logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification") %>%
  fit(Survived ~ Pclass + Age, data = titanic_train)
#note: The above statement is same as fit(set_mode(set_engine(logistic_reg(),"glm"),"classification") , Survived ~ Pclass + Age, data = titanic_train)

model
```




